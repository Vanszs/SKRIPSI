# Gas Fee ML Project Structure

## ğŸ“ Directory Organization

```
gas-ml/
â”œâ”€â”€ src/                    # Core source code
â”‚   â”œâ”€â”€ train.py           # Training pipeline
â”‚   â”œâ”€â”€ stack.py           # Hybrid LSTM-XGBoost model
â”‚   â”œâ”€â”€ infer.py           # Inference engine
â”‚   â”œâ”€â”€ policy.py          # Gas fee policy logic
â”‚   â”œâ”€â”€ features.py        # Feature engineering
â”‚   â”œâ”€â”€ evaluate.py        # Model evaluation
â”‚   â””â”€â”€ fetch.py           # Data fetching from RPC
â”‚
â”œâ”€â”€ models/                 # Production model files (CURRENT)
â”‚   â”œâ”€â”€ lstm.pt            # LSTM weights
â”‚   â”œâ”€â”€ xgb.bin            # XGBoost model
â”‚   â”œâ”€â”€ hybrid_metadata.pkl # Model configuration
â”‚   â”œâ”€â”€ scaler.pkl         # Feature scaler
â”‚   â”œâ”€â”€ target_scaler.pkl  # Target scaler (for denormalization)
â”‚   â”œâ”€â”€ metrics.json       # Evaluation metrics
â”‚   â””â”€â”€ training_info.json # Training metadata
â”‚
â”œâ”€â”€ data/                   # Dataset files
â”‚   â”œâ”€â”€ features.parquet   # Engineered features (4999 blocks)
â”‚   â”œâ”€â”€ blocks_5k.csv      # Raw block data
â”‚   â”œâ”€â”€ selected_features.txt # Feature selection list
â”‚   â””â”€â”€ test/              # Test data for experiments
â”‚
â”œâ”€â”€ cfg/                    # Configuration files
â”‚   â””â”€â”€ exp.yaml           # Experiment hyperparameters
â”‚
â”œâ”€â”€ outputs/                # All generated outputs
â”‚   â”œâ”€â”€ predictions/       # Inference results
â”‚   â”œâ”€â”€ logs/              # Training logs
â”‚   â””â”€â”€ archived_models/   # Previous model versions
â”‚       â”œâ”€â”€ v1_before_normalization/ # First training (37.89% under-est)
â”‚       â””â”€â”€ backup/        # Other backups
â”‚
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ audit/             # Audit reports & fixes
â”‚   â”‚   â”œâ”€â”€ AUDIT_REPORT.md
â”‚   â”‚   â”œâ”€â”€ FIXES_APPLIED.md
â”‚   â”‚   â””â”€â”€ RETRAINING_CHECKLIST.md
â”‚   â”œâ”€â”€ CLI_USAGE.md       # Command-line interface guide
â”‚   â”œâ”€â”€ GPU_TRAINING.md    # GPU setup instructions
â”‚   â””â”€â”€ IMPROVEMENT_STRATEGY.md # Optimization strategies
â”‚
â”œâ”€â”€ notebooks/              # Jupyter notebooks for analysis
â”‚   â”œâ”€â”€ 01_eda_gas_fee.ipynb
â”‚   â”œâ”€â”€ 02_model_evaluation.ipynb
â”‚   â””â”€â”€ 03_xgboost_model_analysis.ipynb
â”‚
â”œâ”€â”€ experiments_comparison/ # Historical experiment tracking
â”‚   â”œâ”€â”€ exp1_lstm_no_temporal/
â”‚   â”œâ”€â”€ exp2_xgb_stacking/
â”‚   â”œâ”€â”€ exp3_tcn/
â”‚   â””â”€â”€ exp4_tft/
â”‚
â”œâ”€â”€ tests/                  # Unit tests
â”‚   â””â”€â”€ test_metrics.py
â”‚
â”œâ”€â”€ cli.py                  # Command-line interface
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ setup_project.ps1      # Setup script
â””â”€â”€ run.ps1                # Quick training script
```

## ğŸ¯ Current Model Status

**Version:** V2 (Production-Ready with Target Normalization)
**Date:** November 10, 2025
**Location:** `models/`

### Performance Metrics
- **MAE:** 1.95 Gwei
- **MAPE:** 2.03% (excellent, baseline 15-25%)
- **RÂ²:** 0.9723
- **Under-estimation Rate:** 10.40% (acceptable for production)
- **Hit Rate @ Îµ=5%:** 88.65%

### Key Improvements Applied
1. âœ… Fixed LSTM placeholder prediction head
2. âœ… Fixed data leakage (shuffle=False)
3. âœ… Target normalization with StandardScaler
4. âœ… Asymmetric loss (2.5x penalty for under-estimation)
5. âœ… Gradient clipping + weight decay
6. âœ… Buffer tuning (25% safety buffer)

## ğŸš€ Quick Start

### Training
```powershell
python src\train.py --cfg cfg\exp.yaml --in data\features.parquet
```

### Inference
```powershell
python cli.py predict
```

### Model Info
```powershell
python cli.py info
```

## ğŸ“Š Model Architecture

**Hybrid Stacking Approach:**
1. **LSTM Feature Extractor**
   - Input: 20-block sequences Ã— 14 features
   - Hidden: 192 units, 2 layers, dropout 0.25
   - Output: Temporal features + initial prediction

2. **XGBoost Final Predictor**
   - Input: LSTM features + original features
   - Trees: 700, max_depth: 6
   - Custom asymmetric objective (2.5x under-penalty)

## ğŸ”§ Configuration

Key parameters in `cfg/exp.yaml`:
- `buffer_multiplier: 1.25` - 25% safety buffer
- `priority_fee_percentile: 0.6` - 60th percentile
- `max_fee_multiplier: 1.8` - Max fee cap
- `sequence_length: 20` - LSTM lookback window

## ğŸ“ˆ Data Flow

```
blocks_5k.csv â†’ features.parquet â†’ normalize â†’ LSTM â†’ XGBoost â†’ predictions
                                      â†“
                                  target_scaler
                                      â†“
                                  denormalize
```

## ğŸ“ For Thesis Documentation

### Critical Files for Analysis
- `models/metrics.json` - Quantitative results
- `models/training_info.json` - Training details
- `docs/audit/AUDIT_REPORT.md` - Bug discovery & resolution
- `outputs/archived_models/` - Model evolution comparison

### Key Findings
- **V1 vs V2:** Slight accuracy decrease (1.49%â†’2.03% MAPE) but 72% reduction in under-estimation (37.89%â†’10.40%)
- **Trade-off:** Production safety prioritized over perfect accuracy
- **GPU Acceleration:** RTX 3050 enables 10-60 min training vs hours on CPU

## âš ï¸ Important Notes

1. **Never shuffle time series data** - Temporal order must be preserved
2. **Always normalize targets** - Critical for LSTM training stability
3. **Asymmetric loss required** - Under-estimation causes transaction failures
4. **Target scaler persistence** - Must save/load target_scaler.pkl for denormalization

## ğŸ“ Maintenance

### Before Retraining
1. Backup current models: `Copy-Item models outputs\archived_models\backup_YYYYMMDD -Recurse`
2. Update `cfg/exp.yaml` if needed
3. Verify GPU availability: `python -c "import torch; print(torch.cuda.is_available())"`

### After Retraining
1. Compare metrics.json with previous version
2. Test inference: `python cli.py predict`
3. Commit changes to git
